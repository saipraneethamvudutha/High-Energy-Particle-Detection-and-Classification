{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\n",
      "Files in Directory: ['.git', 'CNN.ipynb', 'Data Sets', 'FINAL_1.ipynb', 'logs', 'particle_images', 'PPT & Images', 'preprocessed_data.csv', 'README.md', 'Simulation.ipynb', 'wandb', 'xgboost_model.onnx', 'xgboost_optimized.onnx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())  # Prints the directory where Python is looking for the file\n",
    "print(\"Files in Directory:\", os.listdir())  # Lists all files in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_combined = pd.read_csv(\"preprocessed_data.csv\")\n",
    "\n",
    "# Verify the data\n",
    "print(df_combined.head())  # Check the first few rows\n",
    "print(f\"Dataset shape: {df_combined.shape}\")  # Print dataset size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Using TensorFlow DirectML with GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check available GPUs (should list DirectML GPU)\n",
    "print(\"Available Devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Force TensorFlow to use DirectML\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices(\"GPU\")[1], True)\n",
    "print(\"Using TensorFlow DirectML with GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_combined = pd.read_csv(\"preprocessed_data.csv\")\n",
    "\n",
    "# Verify data\n",
    "print(df_combined.head())  # Check the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Create a directory to store generated images\n",
    "os.makedirs(\"particle_images\", exist_ok=True)\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_combined = pd.read_csv(\"preprocessed_data.csv\")\n",
    "\n",
    "# Convert Pandas DataFrame to NumPy arrays (DirectML-compatible)\n",
    "lep_eta = df_combined[\"lep_eta\"].values.astype(np.float32)\n",
    "lep_phi = df_combined[\"lep_phi\"].values.astype(np.float32)\n",
    "lep_pt = df_combined[\"lep_pt\"].values.astype(np.float32)\n",
    "\n",
    "# Function to generate images\n",
    "def generate_image(args):\n",
    "    idx, eta, phi, pt = args\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    scatter = ax.scatter(eta, phi, c=pt, cmap=\"inferno\")\n",
    "\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-np.pi, np.pi)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    plt.savefig(f\"particle_images/{idx}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# **Use multiprocessing to speed up image generation**\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        pool.map(generate_image, zip(range(len(lep_eta)), lep_eta, lep_phi, lep_pt))\n",
    "\n",
    "print(\"✅ Image generation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Processed images 0 to 99\n",
      "Processed images 100 to 199\n",
      "Processed images 200 to 299\n",
      "Processed images 300 to 399\n",
      "Processed images 400 to 499\n",
      "Processed images 500 to 599\n",
      "Processed images 600 to 699\n",
      "Processed images 700 to 799\n",
      "Processed images 800 to 899\n",
      "Processed images 900 to 999\n",
      "Processed images 1000 to 1099\n",
      "Processed images 1100 to 1199\n",
      "Processed images 1200 to 1299\n",
      "Processed images 1300 to 1399\n",
      "Processed images 1400 to 1499\n",
      "Processed images 1500 to 1599\n",
      "Processed images 1600 to 1699\n",
      "Processed images 1700 to 1799\n",
      "Processed images 1800 to 1899\n",
      "Processed images 1900 to 1999\n",
      "Processed images 2000 to 2099\n",
      "Processed images 2100 to 2199\n",
      "Processed images 2200 to 2299\n",
      "Processed images 2300 to 2399\n",
      "Processed images 2400 to 2499\n",
      "Processed images 2500 to 2599\n",
      "Processed images 2600 to 2699\n",
      "Processed images 2700 to 2799\n",
      "Processed images 2800 to 2899\n",
      "Processed images 2900 to 2999\n",
      "Processed images 3000 to 3099\n",
      "Processed images 3100 to 3199\n",
      "Processed images 3200 to 3299\n",
      "Processed images 3300 to 3399\n",
      "Processed images 3400 to 3499\n",
      "Processed images 3500 to 3599\n",
      "Processed images 3600 to 3699\n",
      "Processed images 3700 to 3799\n",
      "Processed images 3800 to 3899\n",
      "Processed images 3900 to 3999\n",
      "Processed images 4000 to 4099\n",
      "Processed images 4100 to 4199\n",
      "Processed images 4200 to 4299\n",
      "Processed images 4300 to 4399\n",
      "Processed images 4400 to 4499\n",
      "Processed images 4500 to 4599\n",
      "Processed images 4600 to 4699\n",
      "Processed images 4700 to 4799\n",
      "Processed images 4800 to 4899\n",
      "Processed images 4900 to 4999\n",
      "Processed images 5000 to 5099\n",
      "Processed images 5100 to 5199\n",
      "Processed images 5200 to 5299\n",
      "Processed images 5300 to 5399\n",
      "Processed images 5400 to 5499\n",
      "Processed images 5500 to 5599\n",
      "Processed images 5600 to 5699\n",
      "Processed images 5700 to 5799\n",
      "Processed images 5800 to 5899\n",
      "Processed images 5900 to 5999\n",
      "Processed images 6000 to 6099\n",
      "Processed images 6100 to 6199\n",
      "Processed images 6200 to 6299\n",
      "Processed images 6300 to 6399\n",
      "Processed images 6400 to 6499\n",
      "Processed images 6500 to 6599\n",
      "Processed images 6600 to 6699\n",
      "Processed images 6700 to 6799\n",
      "Processed images 6800 to 6899\n",
      "Processed images 6900 to 6999\n",
      "Processed images 7000 to 7099\n",
      "Processed images 7100 to 7199\n",
      "Processed images 7200 to 7299\n",
      "Processed images 7300 to 7399\n",
      "Processed images 7400 to 7499\n",
      "Processed images 7500 to 7599\n",
      "Processed images 7600 to 7699\n",
      "Processed images 7700 to 7799\n",
      "Processed images 7800 to 7899\n",
      "Processed images 7900 to 7999\n",
      "Processed images 8000 to 8099\n",
      "Processed images 8100 to 8199\n",
      "Processed images 8200 to 8299\n",
      "Processed images 8300 to 8399\n",
      "Processed images 8400 to 8499\n",
      "Processed images 8500 to 8599\n",
      "Processed images 8600 to 8699\n",
      "Processed images 8700 to 8799\n",
      "Processed images 8800 to 8899\n",
      "Processed images 8900 to 8999\n",
      "Processed images 9000 to 9099\n",
      "Processed images 9100 to 9199\n",
      "Processed images 9200 to 9299\n",
      "Processed images 9300 to 9399\n",
      "Processed images 9400 to 9499\n",
      "Processed images 9500 to 9599\n",
      "Processed images 9600 to 9699\n",
      "Processed images 9700 to 9799\n",
      "Processed images 9800 to 9899\n",
      "Processed images 9900 to 9999\n",
      "Processed images 10000 to 10099\n",
      "Processed images 10100 to 10199\n",
      "Processed images 10200 to 10299\n",
      "Processed images 10300 to 10399\n",
      "Processed images 10400 to 10499\n",
      "Processed images 10500 to 10599\n",
      "Processed images 10600 to 10699\n",
      "Processed images 10700 to 10799\n",
      "Processed images 10800 to 10899\n",
      "Processed images 10900 to 10999\n",
      "Processed images 11000 to 11099\n",
      "Processed images 11100 to 11199\n",
      "Processed images 11200 to 11299\n",
      "Processed images 11300 to 11399\n",
      "Processed images 11400 to 11499\n",
      "Processed images 11500 to 11599\n",
      "Processed images 11600 to 11699\n",
      "Processed images 11700 to 11799\n",
      "Processed images 11800 to 11899\n",
      "Processed images 11900 to 11999\n",
      "Processed images 12000 to 12099\n",
      "Processed images 12100 to 12199\n",
      "Processed images 12200 to 12299\n",
      "Processed images 12300 to 12399\n",
      "Processed images 12400 to 12499\n",
      "Processed images 12500 to 12599\n",
      "Processed images 12600 to 12699\n",
      "Processed images 12700 to 12799\n",
      "Processed images 12800 to 12899\n",
      "Processed images 12900 to 12999\n",
      "Processed images 13000 to 13099\n",
      "Processed images 13100 to 13199\n",
      "Processed images 13200 to 13299\n",
      "Processed images 13300 to 13399\n",
      "Processed images 13400 to 13499\n",
      "Processed images 13500 to 13599\n",
      "Processed images 13600 to 13699\n",
      "Processed images 13700 to 13799\n",
      "Processed images 13800 to 13899\n",
      "Processed images 13900 to 13999\n",
      "Processed images 14000 to 14099\n",
      "Processed images 14100 to 14199\n",
      "Processed images 14200 to 14299\n",
      "Processed images 14300 to 14399\n",
      "Processed images 14400 to 14499\n",
      "Processed images 14500 to 14599\n",
      "Processed images 14600 to 14699\n",
      "Processed images 14700 to 14799\n",
      "Processed images 14800 to 14899\n",
      "Processed images 14900 to 14999\n",
      "Processed images 15000 to 15099\n",
      "Processed images 15100 to 15199\n",
      "Processed images 15200 to 15299\n",
      "Processed images 15300 to 15399\n",
      "Processed images 15400 to 15499\n",
      "Processed images 15500 to 15599\n",
      "Processed images 15600 to 15699\n",
      "Processed images 15700 to 15799\n",
      "Processed images 15800 to 15899\n",
      "Processed images 15900 to 15999\n",
      "Processed images 16000 to 16099\n",
      "Processed images 16100 to 16199\n",
      "Processed images 16200 to 16299\n",
      "Processed images 16300 to 16399\n",
      "Processed images 16400 to 16499\n",
      "Processed images 16500 to 16599\n",
      "Processed images 16600 to 16699\n",
      "Processed images 16700 to 16799\n",
      "Processed images 16800 to 16899\n",
      "Processed images 16900 to 16999\n",
      "Processed images 17000 to 17099\n",
      "Processed images 17100 to 17199\n",
      "Processed images 17200 to 17299\n",
      "Processed images 17300 to 17399\n",
      "Processed images 17400 to 17499\n",
      "Processed images 17500 to 17599\n",
      "Processed images 17600 to 17699\n",
      "Processed images 17700 to 17799\n",
      "Processed images 17800 to 17899\n",
      "Processed images 17900 to 17999\n",
      "Processed images 18000 to 18099\n",
      "Processed images 18100 to 18199\n",
      "Processed images 18200 to 18299\n",
      "Processed images 18300 to 18399\n",
      "Processed images 18400 to 18499\n",
      "Processed images 18500 to 18599\n",
      "Processed images 18600 to 18699\n",
      "Processed images 18700 to 18799\n",
      "Processed images 18800 to 18899\n",
      "Processed images 18900 to 18999\n",
      "Processed images 19000 to 19099\n",
      "Processed images 19100 to 19199\n",
      "Processed images 19200 to 19299\n",
      "Processed images 19300 to 19399\n",
      "Processed images 19400 to 19499\n",
      "Processed images 19500 to 19599\n",
      "Processed images 19600 to 19699\n",
      "Processed images 19700 to 19799\n",
      "Processed images 19800 to 19899\n",
      "Processed images 19900 to 19999\n",
      "Processed images 20000 to 20099\n",
      "Processed images 20100 to 20199\n",
      "Processed images 20200 to 20299\n",
      "Processed images 20300 to 20399\n",
      "Processed images 20400 to 20499\n",
      "Processed images 20500 to 20599\n",
      "Processed images 20600 to 20699\n",
      "Processed images 20700 to 20799\n",
      "Processed images 20800 to 20899\n",
      "Processed images 20900 to 20999\n",
      "Processed images 21000 to 21099\n",
      "Processed images 21100 to 21199\n",
      "Processed images 21200 to 21299\n",
      "Processed images 21300 to 21399\n",
      "Processed images 21400 to 21499\n",
      "Processed images 21500 to 21599\n",
      "Processed images 21600 to 21699\n",
      "Processed images 21700 to 21799\n",
      "Processed images 21800 to 21899\n",
      "Processed images 21900 to 21999\n",
      "Processed images 22000 to 22099\n",
      "Processed images 22100 to 22199\n",
      "Processed images 22200 to 22299\n",
      "Processed images 22300 to 22399\n",
      "Processed images 22400 to 22499\n",
      "Processed images 22500 to 22599\n",
      "Processed images 22600 to 22699\n",
      "Processed images 22700 to 22799\n",
      "Processed images 22800 to 22899\n",
      "Processed images 22900 to 22999\n",
      "Processed images 23000 to 23099\n",
      "Processed images 23100 to 23199\n",
      "Processed images 23200 to 23299\n",
      "Processed images 23300 to 23399\n",
      "Processed images 23400 to 23499\n",
      "Processed images 23500 to 23599\n",
      "Processed images 23600 to 23699\n",
      "Processed images 23700 to 23799\n",
      "Processed images 23800 to 23899\n",
      "Processed images 23900 to 23999\n",
      "Processed images 24000 to 24099\n",
      "Processed images 24100 to 24199\n",
      "Processed images 24200 to 24299\n",
      "Processed images 24300 to 24399\n",
      "Processed images 24400 to 24499\n",
      "Processed images 24500 to 24599\n",
      "Processed images 24600 to 24699\n",
      "Processed images 24700 to 24799\n",
      "Processed images 24800 to 24899\n",
      "Processed images 24900 to 24999\n",
      "Processed images 25000 to 25099\n",
      "Processed images 25100 to 25199\n",
      "Processed images 25200 to 25299\n",
      "Processed images 25300 to 25399\n",
      "Processed images 25400 to 25499\n",
      "Processed images 25500 to 25599\n",
      "Processed images 25600 to 25699\n",
      "Processed images 25700 to 25799\n",
      "Processed images 25800 to 25899\n",
      "Processed images 25900 to 25999\n",
      "Processed images 26000 to 26099\n",
      "Processed images 26100 to 26199\n",
      "Processed images 26200 to 26299\n",
      "Processed images 26300 to 26399\n",
      "Processed images 26400 to 26499\n",
      "Processed images 26500 to 26599\n",
      "Processed images 26600 to 26699\n",
      "Processed images 26700 to 26799\n",
      "Processed images 26800 to 26899\n",
      "Processed images 26900 to 26999\n",
      "Processed images 27000 to 27099\n",
      "Processed images 27100 to 27199\n",
      "Processed images 27200 to 27299\n",
      "Processed images 27300 to 27399\n",
      "Processed images 27400 to 27499\n",
      "Processed images 27500 to 27599\n",
      "Processed images 27600 to 27699\n",
      "Processed images 27700 to 27799\n",
      "Processed images 27800 to 27899\n",
      "Processed images 27900 to 27999\n",
      "Processed images 28000 to 28099\n",
      "Processed images 28100 to 28199\n",
      "Processed images 28200 to 28299\n",
      "Processed images 28300 to 28399\n",
      "Processed images 28400 to 28499\n",
      "Processed images 28500 to 28599\n",
      "Processed images 28600 to 28699\n",
      "Processed images 28700 to 28799\n",
      "Processed images 28800 to 28899\n",
      "Processed images 28900 to 28999\n",
      "Processed images 29000 to 29099\n",
      "Processed images 29100 to 29199\n",
      "Processed images 29200 to 29299\n",
      "Processed images 29300 to 29399\n",
      "Processed images 29400 to 29499\n",
      "Processed images 29500 to 29599\n",
      "Processed images 29600 to 29699\n",
      "Processed images 29700 to 29799\n",
      "Processed images 29800 to 29899\n",
      "Processed images 29900 to 29999\n",
      "Processed images 30000 to 30099\n",
      "Processed images 30100 to 30199\n",
      "Processed images 30200 to 30299\n",
      "Processed images 30300 to 30399\n",
      "Processed images 30400 to 30499\n",
      "Processed images 30500 to 30599\n",
      "Processed images 30600 to 30699\n",
      "Processed images 30700 to 30799\n",
      "Processed images 30800 to 30899\n",
      "Processed images 30900 to 30999\n",
      "Processed images 31000 to 31099\n",
      "Processed images 31100 to 31199\n",
      "Processed images 31200 to 31299\n",
      "Processed images 31300 to 31399\n",
      "Processed images 31400 to 31499\n",
      "Processed images 31500 to 31599\n",
      "Processed images 31600 to 31699\n",
      "Processed images 31700 to 31799\n",
      "Processed images 31800 to 31899\n",
      "Processed images 31900 to 31999\n",
      "Processed images 32000 to 32099\n",
      "Processed images 32100 to 32199\n",
      "Processed images 32200 to 32299\n",
      "Processed images 32300 to 32399\n",
      "Processed images 32400 to 32499\n",
      "Processed images 32500 to 32599\n",
      "Processed images 32600 to 32699\n",
      "Processed images 32700 to 32799\n",
      "Processed images 32800 to 32899\n",
      "Processed images 32900 to 32999\n",
      "Processed images 33000 to 33099\n",
      "Processed images 33100 to 33199\n",
      "Processed images 33200 to 33299\n",
      "Processed images 33300 to 33399\n",
      "Processed images 33400 to 33499\n",
      "Processed images 33500 to 33599\n",
      "Processed images 33600 to 33699\n",
      "Processed images 33700 to 33799\n",
      "Processed images 33800 to 33899\n",
      "Processed images 33900 to 33999\n",
      "Processed images 34000 to 34099\n",
      "Processed images 34100 to 34199\n",
      "Processed images 34200 to 34299\n",
      "Processed images 34300 to 34399\n",
      "Processed images 34400 to 34499\n",
      "Processed images 34500 to 34599\n",
      "Processed images 34600 to 34699\n",
      "Processed images 34700 to 34799\n",
      "Processed images 34800 to 34899\n",
      "Processed images 34900 to 34999\n",
      "Processed images 35000 to 35099\n",
      "Processed images 35100 to 35199\n",
      "Processed images 35200 to 35299\n",
      "Processed images 35300 to 35399\n",
      "Processed images 35400 to 35499\n",
      "Processed images 35500 to 35599\n",
      "Processed images 35600 to 35699\n",
      "Processed images 35700 to 35799\n",
      "Processed images 35800 to 35899\n",
      "Processed images 35900 to 35999\n",
      "Processed images 36000 to 36099\n",
      "Processed images 36100 to 36199\n",
      "Processed images 36200 to 36299\n",
      "Processed images 36300 to 36399\n",
      "Processed images 36400 to 36499\n",
      "Processed images 36500 to 36599\n",
      "Processed images 36600 to 36699\n",
      "Processed images 36700 to 36799\n",
      "Processed images 36800 to 36899\n",
      "Processed images 36900 to 36999\n",
      "Processed images 37000 to 37099\n",
      "Processed images 37100 to 37199\n",
      "Processed images 37200 to 37299\n",
      "Processed images 37300 to 37399\n",
      "Processed images 37400 to 37499\n",
      "Processed images 37500 to 37599\n",
      "Processed images 37600 to 37699\n",
      "Processed images 37700 to 37799\n",
      "Processed images 37800 to 37899\n",
      "Processed images 37900 to 37999\n",
      "Processed images 38000 to 38099\n",
      "Processed images 38100 to 38199\n",
      "Processed images 38200 to 38299\n",
      "Processed images 38300 to 38399\n",
      "Processed images 38400 to 38499\n",
      "Processed images 38500 to 38599\n",
      "Processed images 38600 to 38699\n",
      "Processed images 38700 to 38799\n",
      "Processed images 38800 to 38899\n",
      "Processed images 38900 to 38999\n",
      "Processed images 39000 to 39099\n",
      "Processed images 39100 to 39199\n",
      "Processed images 39200 to 39299\n",
      "Processed images 39300 to 39399\n",
      "Processed images 39400 to 39499\n",
      "Processed images 39500 to 39599\n",
      "Processed images 39600 to 39699\n",
      "Processed images 39700 to 39799\n",
      "Processed images 39800 to 39899\n",
      "Processed images 39900 to 39999\n",
      "Processed images 40000 to 40099\n",
      "Processed images 40100 to 40199\n",
      "Processed images 40200 to 40299\n",
      "Processed images 40300 to 40399\n",
      "Processed images 40400 to 40499\n",
      "Processed images 40500 to 40599\n",
      "Processed images 40600 to 40699\n",
      "Processed images 40700 to 40799\n",
      "Processed images 40800 to 40899\n",
      "Processed images 40900 to 40999\n",
      "Processed images 41000 to 41099\n",
      "Processed images 41100 to 41199\n",
      "Processed images 41200 to 41299\n",
      "Processed images 41300 to 41399\n",
      "Processed images 41400 to 41499\n",
      "Processed images 41500 to 41599\n",
      "Processed images 41600 to 41699\n",
      "Processed images 41700 to 41799\n",
      "Processed images 41800 to 41899\n",
      "Processed images 41900 to 41999\n",
      "Processed images 42000 to 42099\n",
      "Processed images 42100 to 42199\n",
      "Processed images 42200 to 42299\n",
      "Processed images 42300 to 42399\n",
      "Processed images 42400 to 42499\n",
      "Processed images 42500 to 42599\n",
      "Processed images 42600 to 42699\n",
      "Processed images 42700 to 42799\n",
      "Processed images 42800 to 42899\n",
      "Processed images 42900 to 42999\n",
      "Processed images 43000 to 43099\n",
      "Processed images 43100 to 43199\n",
      "Processed images 43200 to 43299\n",
      "Processed images 43300 to 43399\n",
      "Processed images 43400 to 43499\n",
      "Processed images 43500 to 43599\n",
      "Processed images 43600 to 43699\n",
      "Processed images 43700 to 43799\n",
      "Processed images 43800 to 43899\n",
      "Processed images 43900 to 43999\n",
      "Processed images 44000 to 44099\n",
      "Processed images 44100 to 44199\n",
      "Processed images 44200 to 44299\n",
      "Processed images 44300 to 44399\n",
      "Processed images 44400 to 44499\n",
      "Processed images 44500 to 44599\n",
      "Processed images 44600 to 44699\n",
      "Processed images 44700 to 44799\n",
      "Processed images 44800 to 44899\n",
      "Processed images 44900 to 44999\n",
      "Processed images 45000 to 45099\n",
      "Processed images 45100 to 45199\n",
      "Processed images 45200 to 45299\n",
      "Processed images 45300 to 45399\n",
      "Processed images 45400 to 45499\n",
      "Processed images 45500 to 45599\n",
      "Processed images 45600 to 45699\n",
      "Processed images 45700 to 45799\n",
      "Processed images 45800 to 45899\n",
      "Processed images 45900 to 45999\n",
      "Processed images 46000 to 46099\n",
      "Processed images 46100 to 46199\n",
      "Processed images 46200 to 46299\n",
      "Processed images 46300 to 46399\n",
      "Processed images 46400 to 46499\n",
      "Processed images 46500 to 46599\n",
      "Processed images 46600 to 46699\n",
      "Processed images 46700 to 46799\n",
      "Processed images 46800 to 46899\n",
      "Processed images 46900 to 46999\n",
      "Processed images 47000 to 47099\n",
      "Processed images 47100 to 47199\n",
      "Processed images 47200 to 47299\n",
      "Processed images 47300 to 47399\n",
      "Processed images 47400 to 47499\n",
      "Processed images 47500 to 47599\n",
      "Processed images 47600 to 47699\n",
      "Processed images 47700 to 47799\n",
      "Processed images 47800 to 47899\n",
      "Processed images 47900 to 47999\n",
      "Processed images 48000 to 48099\n",
      "Processed images 48100 to 48199\n",
      "Processed images 48200 to 48299\n",
      "Processed images 48300 to 48399\n",
      "Processed images 48400 to 48499\n",
      "Processed images 48500 to 48599\n",
      "Processed images 48600 to 48699\n",
      "Processed images 48700 to 48799\n",
      "Processed images 48800 to 48899\n",
      "Processed images 48900 to 48999\n",
      "Processed images 49000 to 49099\n",
      "Processed images 49100 to 49199\n",
      "Processed images 49200 to 49299\n",
      "Processed images 49300 to 49399\n",
      "Processed images 49400 to 49499\n",
      "Processed images 49500 to 49599\n",
      "Processed images 49600 to 49699\n",
      "Processed images 49700 to 49799\n",
      "Processed images 49800 to 49899\n",
      "Processed images 49900 to 49999\n",
      "✅ Image generation completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set necessary environment variables\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "# Disable eager execution to avoid early initialization\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "\n",
    "# Only then import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make sure TF doesn't eagerly initialize the context\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Configure memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"Found GPU devices: {physical_devices}\")\n",
    "    for device in physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(f\"Memory growth enabled for {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "# Your remaining imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df_combined = pd.read_csv(\"preprocessed_data.csv\")\n",
    "\n",
    "# Reduce dataset: Sample 50,000 images\n",
    "df_sampled = df_combined.sample(n=50000, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays - don't use TensorFlow tensors for the matplotlib workflow\n",
    "lep_eta = df_sampled[\"lep_eta\"].values\n",
    "lep_phi = df_sampled[\"lep_phi\"].values\n",
    "lep_pt = df_sampled[\"lep_pt\"].values\n",
    "\n",
    "# Create directory for images\n",
    "os.makedirs(\"particle_images\", exist_ok=True)\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 100\n",
    "\n",
    "# Process in batches without TensorFlow\n",
    "for i in range(0, len(lep_eta), batch_size):\n",
    "    end_idx = min(i + batch_size, len(lep_eta))\n",
    "    \n",
    "    for j in range(i, end_idx):\n",
    "        # Generate and save image using matplotlib directly\n",
    "        fig, ax = plt.subplots(figsize=(2, 2))\n",
    "        scatter = ax.scatter(lep_eta[j], lep_phi[j], c=lep_pt[j], cmap=\"inferno\")\n",
    "        \n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-np.pi, np.pi)\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "        plt.savefig(f\"particle_images/{j}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Processed images {i} to {end_idx-1}\")\n",
    "\n",
    "print(\"✅ Image generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50001/50001 [04:47<00:00, 173.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Image verification and conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure images are correctly stored in class subdirectories (e.g., particle_images/class_name/)\n",
    "image_dir = \"particle_images/\"\n",
    "target_dir = \"particle_images/processed/\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Convert any non-PNG images to PNG format\n",
    "for img_name in tqdm(os.listdir(image_dir)):\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "\n",
    "    # Ignore non-image files\n",
    "    if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        continue\n",
    "\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue  # Skip corrupted images\n",
    "\n",
    "    # Convert & Save as PNG\n",
    "    save_path = os.path.join(target_dir, f\"{os.path.splitext(img_name)[0]}.png\")\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "print(\"✅ Image verification and conversion completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50001/50001 [00:20<00:00, 2405.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Image organization completed: Files moved into class-labeled folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "source_dir = \"particle_images/\"\n",
    "processed_dir = \"particle_images/processed/\"\n",
    "class_0_dir = os.path.join(processed_dir, \"class_0\")\n",
    "class_1_dir = os.path.join(processed_dir, \"class_1\")\n",
    "\n",
    "# Ensure class directories exist\n",
    "os.makedirs(class_0_dir, exist_ok=True)\n",
    "os.makedirs(class_1_dir, exist_ok=True)\n",
    "\n",
    "# Move images into class folders\n",
    "for idx, img_name in enumerate(tqdm(os.listdir(source_dir))):\n",
    "    img_path = os.path.join(source_dir, img_name)\n",
    "\n",
    "    # Only process valid image files\n",
    "    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        if idx % 2 == 0:\n",
    "            shutil.move(img_path, os.path.join(class_0_dir, img_name))  # Assign half images to class_0\n",
    "        else:\n",
    "            shutil.move(img_path, os.path.join(class_1_dir, img_name))  # Assign half to class_1\n",
    "\n",
    "print(\"✅ Image organization completed: Files moved into class-labeled folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "✅ Dataset Loaded Successfully & Using GPU-0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Ensure TensorFlow DirectML uses GPU-0\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "tf.device('/GPU:0')\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\particle_images\\processed\"  # Use the corrected folder\n",
    "\n",
    "# Load dataset\n",
    "batch_size = 32\n",
    "img_size = (64, 64)  # Resize for CNN\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Prefetch for performance boost\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"✅ Dataset Loaded Successfully & Using GPU-0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683,458\n",
      "Trainable params: 683,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 20s 15ms/step - loss: 0.7775 - accuracy: 0.4986 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 0.6932 - accuracy: 0.5014 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.4979 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "✅ CNN Training Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Force GPU usage\n",
    "with tf.device('/GPU:0'):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(2, activation=\"softmax\")  # Assuming binary classification (signal vs. background)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "# Train Model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "print(\"✅ CNN Training Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using weights and baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvuduthasaipraneetham\u001b[0m (\u001b[33mpraneetham\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.6s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praneetham/particle_cnn/runs/5zcvrrkr' target=\"_blank\">initial_training</a></strong> to <a href='https://wandb.ai/praneetham/particle_cnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praneetham/particle_cnn' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praneetham/particle_cnn/runs/5zcvrrkr' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn/runs/5zcvrrkr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is deprecated and will be removed in a future release. Please use the WandbMetricsLogger, WandbModelCheckpoint, and WandbEvalCallback callbacks instead. See https://docs.wandb.ai/guides/integrations/keras for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.8113 - accuracy: 0.5010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.8112 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_233740-5zcvrrkr\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 18s 15ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>accuracy</td><td>█▇▄▇▆▁▅▂▃█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▁▁▄▆▄▅▅▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.4925</td></tr><tr><td>Test Loss</td><td>0.69324</td></tr><tr><td>accuracy</td><td>0.50102</td></tr><tr><td>best_epoch</td><td>2</td></tr><tr><td>best_val_loss</td><td>0.6932</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.69317</td></tr><tr><td>val_accuracy</td><td>0.4925</td></tr><tr><td>val_loss</td><td>0.69324</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">initial_training</strong> at: <a href='https://wandb.ai/praneetham/particle_cnn/runs/5zcvrrkr' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn/runs/5zcvrrkr</a><br> View project at: <a href='https://wandb.ai/praneetham/particle_cnn' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn</a><br>Synced 5 W&B file(s), 1 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250228_233740-5zcvrrkr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"particle_cnn\", name=\"initial_training\")\n",
    "\n",
    "# Define hyperparameters\n",
    "config = wandb.config\n",
    "config.batch_size = 32\n",
    "config.epochs = 10\n",
    "config.learning_rate = 0.001\n",
    "\n",
    "# Load dataset (replace with your dataset path)\n",
    "dataset_path = \"particle_images/processed\"\n",
    "\n",
    "# Load dataset with GPU acceleration\n",
    "with tf.device('/GPU:0'):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=42,\n",
    "        image_size=(64, 64),\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=42,\n",
    "        image_size=(64, 64),\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "# Define CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model with W&B logging\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=[wandb.keras.WandbCallback()]  # W&B auto-logs training metrics\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(val_ds)\n",
    "wandb.log({\"Test Accuracy\": test_acc, \"Test Loss\": test_loss})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tnxxwrhl' target=\"_blank\">optuna_sweep</a></strong> to <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tnxxwrhl' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning/runs/tnxxwrhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:41:34,821] A new study created in memory with name: no-name-063989ed-1843-4cac-8581-1fbb6efaaa7e\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.2, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "  5/625 [..............................] - ETA: 26s - loss: 10.1237 - accuracy: 0.4375WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_end` time: 0.0222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_end` time: 0.0222s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/625 [============================>.] - ETA: 0s - loss: 0.8720 - accuracy: 0.4985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 29s 45ms/step - loss: 0.8717 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.6931 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 29s 45ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "157/157 [==============================] - 3s 16ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:46:37,358] Trial 0 finished with value: 0.492499977350235 and parameters: {'learning_rate': 4.3767651299882864e-05, 'batch_size': 64, 'dropout_rate': 0.403598174016896, 'num_filters': 128}. Best is trial 0 with value: 0.492499977350235.\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.2, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.5521 - accuracy: 0.4989 - val_loss: 0.6938 - val_accuracy: 0.4938\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6935 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.4926\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6937 - accuracy: 0.4989 - val_loss: 0.6936 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6936 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 0.6933 - accuracy: 0.5038 - val_loss: 0.6937 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6934 - accuracy: 0.4979 - val_loss: 0.6935 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6933 - accuracy: 0.5001 - val_loss: 0.6935 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6933 - accuracy: 0.4974 - val_loss: 0.6935 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6934 - accuracy: 0.4999 - val_loss: 0.6936 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 0.6933 - accuracy: 0.5003 - val_loss: 0.6937 - val_accuracy: 0.4925\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6937 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:49:41,997] Trial 1 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.007969935687377397, 'batch_size': 32, 'dropout_rate': 0.285663045370641, 'num_filters': 32}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 21s 8ms/step - loss: 0.9596 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4928\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 20s 8ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 22s 9ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 22s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 22s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6954 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:53:43,875] Trial 2 finished with value: 0.492499977350235 and parameters: {'learning_rate': 3.735369036477264e-05, 'batch_size': 16, 'dropout_rate': 0.4509449438296947, 'num_filters': 32}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "  5/625 [..............................] - ETA: 26s - loss: 70.8063 - accuracy: 0.4938 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0179s vs `on_train_batch_end` time: 0.0235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0179s vs `on_train_batch_end` time: 0.0235s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 29s 45ms/step - loss: 1.3120 - accuracy: 0.5023 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 29s 45ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:58:49,062] Trial 3 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.0013855892395864, 'batch_size': 64, 'dropout_rate': 0.2669516523649461, 'num_filters': 128}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "   6/1250 [..............................] - ETA: 30s - loss: 59.0987 - accuracy: 0.4844 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0107s vs `on_train_batch_end` time: 0.0151s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0107s vs `on_train_batch_end` time: 0.0151s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.9875 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:05:17,931] Trial 4 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.0010008937311786733, 'batch_size': 32, 'dropout_rate': 0.31488142976998557, 'num_filters': 128}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8269 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:09:34,125] Trial 5 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.00021267793011391816, 'batch_size': 32, 'dropout_rate': 0.3948915583074198, 'num_filters': 64}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.9251 - accuracy: 0.5010 - val_loss: 0.6935 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 27s 11ms/step - loss: 0.6933 - accuracy: 0.4994 - val_loss: 0.6935 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.4981 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 29s 12ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 29s 11ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 29s 11ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6934 - val_accuracy: 0.4925\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.6934 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:14:37,062] Trial 6 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.002715832612267281, 'batch_size': 16, 'dropout_rate': 0.33910578082345993, 'num_filters': 64}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "623/625 [============================>.] - ETA: 0s - loss: 1.5975 - accuracy: 0.4984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 15s 23ms/step - loss: 1.5946 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.5076\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 15s 24ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6931 - val_accuracy: 0.5075\n",
      "Epoch 6/10\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250228_234134-tnxxwrhl\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 18s 29ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6931 - val_accuracy: 0.5075\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.6932 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:17:25,578] Trial 7 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.005116965166153083, 'batch_size': 64, 'dropout_rate': 0.4525372532373076, 'num_filters': 32}. Best is trial 0 with value: 0.492499977350235.\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "C:\\Users\\vudut\\AppData\\Local\\Temp\\ipykernel_29908\\4024133043.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.2, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 37s 15ms/step - loss: 0.8053 - accuracy: 0.5020 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 32s 13ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 30s 12ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 30s 12ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 28s 11ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6933 - val_accuracy: 0.4925\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.6933 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:23:27,788] Trial 8 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.0016747418044654263, 'batch_size': 16, 'dropout_rate': 0.45323760186571704, 'num_filters': 64}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 2 classes.\n",
      "Using 40000 files for training.\n",
      "Found 50000 files belonging to 2 classes.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 24s 9ms/step - loss: 1.1828 - accuracy: 0.4970 - val_loss: 0.6944 - val_accuracy: 0.4925\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 24s 9ms/step - loss: 0.6934 - accuracy: 0.5001 - val_loss: 0.6943 - val_accuracy: 0.4925\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6934 - accuracy: 0.4971 - val_loss: 0.6943 - val_accuracy: 0.4925\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 22s 9ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6941 - val_accuracy: 0.4925\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 22s 9ms/step - loss: 0.6934 - accuracy: 0.4983 - val_loss: 0.6940 - val_accuracy: 0.4925\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 23s 9ms/step - loss: 0.6934 - accuracy: 0.4984 - val_loss: 0.6940 - val_accuracy: 0.4925\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 24s 9ms/step - loss: 0.6934 - accuracy: 0.4974 - val_loss: 0.6937 - val_accuracy: 0.4925\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.6934 - accuracy: 0.4990 - val_loss: 0.6940 - val_accuracy: 0.4925\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 24s 10ms/step - loss: 0.6934 - accuracy: 0.4978 - val_loss: 0.6940 - val_accuracy: 0.4925\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 24s 9ms/step - loss: 0.6934 - accuracy: 0.5004 - val_loss: 0.6943 - val_accuracy: 0.4925\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.6943 - accuracy: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:27:42,458] Trial 9 finished with value: 0.492499977350235 and parameters: {'learning_rate': 0.007535988281920984, 'batch_size': 16, 'dropout_rate': 0.26424947679762373, 'num_filters': 32}. Best is trial 0 with value: 0.492499977350235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 4.3767651299882864e-05, 'batch_size': 64, 'dropout_rate': 0.403598174016896, 'num_filters': 128}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▆▆▆▆▃██▂▄▄▆▆▆▆▄▆▄▃▅▄▃▄▆▆▆▃▁▂▃▃▄▆▄▆▄▃▄▄▃▃</td></tr><tr><td>epoch</td><td>▂▃▆▁▂▆▇█▂▄█▁▃▄█▄▆▇█▁▃▄▅▆▆█▁▃▅▆▄▅▆▇▁▃▇▁▅▆</td></tr><tr><td>loss</td><td>▁▁▁▁█▁▁▁▁▁▃▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▅▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▄▄▃▃▁▁▁▁▁▁▁▂▁▂▁▁▂▁▂▂▂▂▂▁▁▁▂▂▂▂▂▂██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation Accuracy</td><td>0.4925</td></tr><tr><td>accuracy</td><td>0.50042</td></tr><tr><td>best_epoch</td><td>5</td></tr><tr><td>best_val_loss</td><td>0.69313</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.69335</td></tr><tr><td>val_accuracy</td><td>0.4925</td></tr><tr><td>val_loss</td><td>0.69432</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">optuna_sweep</strong> at: <a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tnxxwrhl' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning/runs/tnxxwrhl</a><br> View project at: <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning</a><br>Synced 5 W&B file(s), 10 media file(s), 18 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250228_234134-tnxxwrhl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "import optuna\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"particle_cnn_tuning\", name=\"optuna_sweep\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"particle_images/processed\"\n",
    "\n",
    "def build_and_train_model(trial):\n",
    "    # Define hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.2, 0.5)\n",
    "    num_filters = trial.suggest_categorical(\"num_filters\", [32, 64, 128])\n",
    "\n",
    "    with tf.device('/GPU:0'):  # Use GPU for faster training\n",
    "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            dataset_path,\n",
    "            validation_split=0.2,\n",
    "            subset=\"training\",\n",
    "            seed=42,\n",
    "            image_size=(64, 64),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            dataset_path,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=42,\n",
    "            image_size=(64, 64),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    # Build CNN model\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(num_filters, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(num_filters * 2, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(num_filters * 2, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train model with W&B logging\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10,\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    _, val_acc = model.evaluate(val_ds)\n",
    "    wandb.log({\"Validation Accuracy\": val_acc})\n",
    "    \n",
    "    return val_acc  # Optuna maximizes this metric\n",
    "\n",
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")  # Maximize validation accuracy\n",
    "study.optimize(build_and_train_model, n_trials=10)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# Save best model hyperparameters to W&B\n",
    "wandb.config.update(study.best_params)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 4.3767651299882864e-05, 'batch_size': 64, 'dropout_rate': 0.403598174016896, 'num_filters': 128}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"learning_rate\": 4.3767651299882864e-05,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout_rate\": 0.403598174016896,\n",
    "    \"num_filters\": 128\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\wandb\\run-20250301_005735-tecqriof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tecqriof' target=\"_blank\">final_training</a></strong> to <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tecqriof' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning/runs/tecqriof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 1 classes.\n",
      "Using 80000 files for training.\n",
      "Found 100000 files belonging to 1 classes.\n",
      "Using 20000 files for validation.\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 65s 51ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 60s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 64s 51ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "313/313 - 7s - loss: 0.0000e+00 - accuracy: 1.0000 - 7s/epoch - 23ms/step\n",
      "\n",
      "🎯 Final Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁█████████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>1</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0</td></tr><tr><td>val_accuracy</td><td>1</td></tr><tr><td>val_loss</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final_training</strong> at: <a href='https://wandb.ai/praneetham/particle_cnn_tuning/runs/tecqriof' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning/runs/tecqriof</a><br> View project at: <a href='https://wandb.ai/praneetham/particle_cnn_tuning' target=\"_blank\">https://wandb.ai/praneetham/particle_cnn_tuning</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250301_005735-tecqriof\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# ✅ Enable TensorFlow DirectML (Check if GPU is detected)\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    print(f\"✅ Using GPU: {physical_devices[0]}\")  # No set_memory_growth to avoid error\n",
    "\n",
    "# ✅ Initialize Weights & Biases (W&B) for tracking\n",
    "wandb.init(project=\"particle_cnn_tuning\", name=\"final_training\", sync_tensorboard=True)\n",
    "\n",
    "# ✅ Load Image Dataset\n",
    "dataset_path = \"particle_images\"  # Ensure this folder contains the images\n",
    "batch_size = 64  # From best hyperparameters\n",
    "img_size = (64, 64)\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# ✅ Apply performance optimizations\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# ✅ Best Hyperparameters (from Optuna tuning)\n",
    "best_params = {\n",
    "    \"learning_rate\": 4.3767651299882864e-05,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout_rate\": 0.403598174016896,\n",
    "    \"num_filters\": 128\n",
    "}\n",
    "\n",
    "# ✅ Define CNN Model with Optimized Hyperparameters\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(best_params[\"num_filters\"], (3, 3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(best_params[\"num_filters\"] * 2, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(best_params[\"num_filters\"] * 4, (3, 3), activation=\"relu\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(best_params[\"dropout_rate\"]),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(2, activation=\"softmax\")  # Binary classification\n",
    "])\n",
    "\n",
    "# ✅ Compile Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params[\"learning_rate\"]),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ✅ Train Model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[wandb.keras.WandbCallback()]  # ✅ Track training with W&B\n",
    ")\n",
    "\n",
    "# ✅ Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(val_ds, verbose=2)\n",
    "print(f\"\\n🎯 Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ✅ Save Trained Model\n",
    "model.save(\"particle_cnn_model.h5\")\n",
    "wandb.finish()  # ✅ Close W&B session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfamd",
   "language": "python",
   "name": "tfamd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
