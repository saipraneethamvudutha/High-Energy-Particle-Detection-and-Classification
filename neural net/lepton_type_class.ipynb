{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a76e6d",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c895583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7097f",
   "metadata": {},
   "source": [
    "# Loading and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573b0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\preprocessed_data.csv\")\n",
    "\n",
    "# Fill missing values in jet_pt (mean imputation)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['jet_pt'] = imputer.fit_transform(df[['jet_pt']])\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['lep_type'])  # Features\n",
    "y = df['lep_type']  # Target (can be changed as needed)\n",
    "\n",
    "# Label encode target if it's categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762c2bf",
   "metadata": {},
   "source": [
    "# Defining basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30dbb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18683/18683 [==============================] - 82s 4ms/step - loss: 0.6721 - accuracy: 0.6120 - val_loss: 0.6550 - val_accuracy: 0.6206\n",
      "Epoch 2/10\n",
      "18683/18683 [==============================] - 96s 5ms/step - loss: 0.6526 - accuracy: 0.6219 - val_loss: 0.6506 - val_accuracy: 0.6224\n",
      "Epoch 3/10\n",
      "18683/18683 [==============================] - 94s 5ms/step - loss: 0.6489 - accuracy: 0.6236 - val_loss: 0.6488 - val_accuracy: 0.6233\n",
      "Epoch 4/10\n",
      "18683/18683 [==============================] - 77s 4ms/step - loss: 0.6475 - accuracy: 0.6245 - val_loss: 0.6472 - val_accuracy: 0.6249\n",
      "Epoch 5/10\n",
      "18683/18683 [==============================] - 78s 4ms/step - loss: 0.6468 - accuracy: 0.6251 - val_loss: 0.6464 - val_accuracy: 0.6253\n",
      "Epoch 6/10\n",
      "18683/18683 [==============================] - 77s 4ms/step - loss: 0.6459 - accuracy: 0.6256 - val_loss: 0.6460 - val_accuracy: 0.6255\n",
      "Epoch 7/10\n",
      "18683/18683 [==============================] - 78s 4ms/step - loss: 0.6452 - accuracy: 0.6260 - val_loss: 0.6448 - val_accuracy: 0.6263\n",
      "Epoch 8/10\n",
      "18683/18683 [==============================] - 78s 4ms/step - loss: 0.6444 - accuracy: 0.6265 - val_loss: 0.6444 - val_accuracy: 0.6259\n",
      "Epoch 9/10\n",
      "18683/18683 [==============================] - 77s 4ms/step - loss: 0.6439 - accuracy: 0.6267 - val_loss: 0.6440 - val_accuracy: 0.6261\n",
      "Epoch 10/10\n",
      "18683/18683 [==============================] - 77s 4ms/step - loss: 0.6437 - accuracy: 0.6269 - val_loss: 0.6431 - val_accuracy: 0.6271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b0ad0ba470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Network\n",
    "nn = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "nn.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(X_train, y_train, epochs=10, batch_size=512, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90435e2",
   "metadata": {},
   "source": [
    "# One hot encoding the labels and normalizing in order to fasten the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6801 - accuracy: 0.6021 - val_loss: 0.6557 - val_accuracy: 0.6192\n",
      "Epoch 2/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6578 - accuracy: 0.6184 - val_loss: 0.6533 - val_accuracy: 0.6207\n",
      "Epoch 3/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6551 - accuracy: 0.6203 - val_loss: 0.6498 - val_accuracy: 0.6240\n",
      "Epoch 4/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6535 - accuracy: 0.6212 - val_loss: 0.6497 - val_accuracy: 0.6230\n",
      "Epoch 5/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6523 - accuracy: 0.6220 - val_loss: 0.6476 - val_accuracy: 0.6251\n",
      "Epoch 6/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6512 - accuracy: 0.6226 - val_loss: 0.6468 - val_accuracy: 0.6252\n",
      "Epoch 7/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6503 - accuracy: 0.6230 - val_loss: 0.6456 - val_accuracy: 0.6263\n",
      "Epoch 8/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6497 - accuracy: 0.6234 - val_loss: 0.6449 - val_accuracy: 0.6266\n",
      "Epoch 9/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6492 - accuracy: 0.6238 - val_loss: 0.6447 - val_accuracy: 0.6273\n",
      "Epoch 10/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6487 - accuracy: 0.6243 - val_loss: 0.6440 - val_accuracy: 0.6282\n",
      "Epoch 11/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6481 - accuracy: 0.6249 - val_loss: 0.6439 - val_accuracy: 0.6284\n",
      "Epoch 12/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6476 - accuracy: 0.6253 - val_loss: 0.6428 - val_accuracy: 0.6290\n",
      "Epoch 13/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6471 - accuracy: 0.6256 - val_loss: 0.6423 - val_accuracy: 0.6291\n",
      "Epoch 14/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6467 - accuracy: 0.6258 - val_loss: 0.6416 - val_accuracy: 0.6296\n",
      "Epoch 15/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6463 - accuracy: 0.6260 - val_loss: 0.6413 - val_accuracy: 0.6292\n",
      "Epoch 16/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6460 - accuracy: 0.6262 - val_loss: 0.6412 - val_accuracy: 0.6299\n",
      "Epoch 17/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6457 - accuracy: 0.6264 - val_loss: 0.6410 - val_accuracy: 0.6294\n",
      "Epoch 18/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6454 - accuracy: 0.6266 - val_loss: 0.6407 - val_accuracy: 0.6288\n",
      "Epoch 19/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6452 - accuracy: 0.6267 - val_loss: 0.6411 - val_accuracy: 0.6292\n",
      "Epoch 20/30\n",
      "9342/9342 [==============================] - 22s 2ms/step - loss: 0.6451 - accuracy: 0.6268 - val_loss: 0.6401 - val_accuracy: 0.6299\n",
      "Epoch 21/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6449 - accuracy: 0.6268 - val_loss: 0.6404 - val_accuracy: 0.6297\n",
      "Epoch 22/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6447 - accuracy: 0.6269 - val_loss: 0.6403 - val_accuracy: 0.6298\n",
      "Epoch 23/30\n",
      "9342/9342 [==============================] - 23s 2ms/step - loss: 0.6445 - accuracy: 0.6269 - val_loss: 0.6407 - val_accuracy: 0.6294\n",
      "93411/93411 [==============================] - 146s 2ms/step - loss: 0.6398 - accuracy: 0.6304\n",
      "Test accuracy: 0.630399763584137\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels (instead of sparse categorical loss)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bf727",
   "metadata": {},
   "source": [
    "# Defining the next iteration of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da81800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "nn = Sequential([\n",
    "    Dense(256, input_dim=X_train_scaled.shape[1], activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "           loss='categorical_crossentropy', \n",
    "           metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b67e72",
   "metadata": {},
   "source": [
    "# Fitting and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "nn.fit(X_train_scaled, y_train_cat, \n",
    "       epochs=30, \n",
    "       batch_size=1024, \n",
    "       validation_split=0.2, \n",
    "       callbacks=[early_stop], \n",
    "       verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = nn.evaluate(X_test_scaled, y_test_cat)\n",
    "print(\"Test accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba16eb0",
   "metadata": {},
   "source": [
    "# Saving the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a44ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save(\"structured.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1394cb",
   "metadata": {},
   "source": [
    "# Again repeating the same steps from labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "336278/336278 [==============================] - 1041s 3ms/step - loss: 0.6691 - accuracy: 0.6091 - val_loss: 0.6562 - val_accuracy: 0.6199 - lr: 5.0000e-04\n",
      "Epoch 2/10\n",
      "336278/336278 [==============================] - 1037s 3ms/step - loss: 0.6614 - accuracy: 0.6152 - val_loss: 0.6557 - val_accuracy: 0.6216 - lr: 5.0000e-04\n",
      "Epoch 3/10\n",
      "336278/336278 [==============================] - 1048s 3ms/step - loss: 0.6601 - accuracy: 0.6161 - val_loss: 0.6542 - val_accuracy: 0.6232 - lr: 5.0000e-04\n",
      "Epoch 4/10\n",
      "336278/336278 [==============================] - 1050s 3ms/step - loss: 0.6595 - accuracy: 0.6166 - val_loss: 0.6536 - val_accuracy: 0.6226 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "336278/336278 [==============================] - 1050s 3ms/step - loss: 0.6590 - accuracy: 0.6170 - val_loss: 0.6520 - val_accuracy: 0.6234 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "336278/336278 [==============================] - 1049s 3ms/step - loss: 0.6586 - accuracy: 0.6172 - val_loss: 0.6534 - val_accuracy: 0.6237 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "336278/336278 [==============================] - 1049s 3ms/step - loss: 0.6585 - accuracy: 0.6173 - val_loss: 0.6513 - val_accuracy: 0.6236 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "336278/336278 [==============================] - 1048s 3ms/step - loss: 0.6583 - accuracy: 0.6176 - val_loss: 0.6544 - val_accuracy: 0.6228 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "336278/336278 [==============================] - 1046s 3ms/step - loss: 0.6582 - accuracy: 0.6174 - val_loss: 0.6697 - val_accuracy: 0.6240 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "336275/336278 [============================>.] - ETA: 0s - loss: 0.6580 - accuracy: 0.6176\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "336278/336278 [==============================] - 1051s 3ms/step - loss: 0.6580 - accuracy: 0.6176 - val_loss: 0.6673 - val_accuracy: 0.6224 - lr: 5.0000e-04\n",
      "93411/93411 [==============================] - 161s 2ms/step - loss: 0.6664 - accuracy: 0.6229\n",
      "Test accuracy: 0.6228587627410889\n",
      "Model saved as 'improved_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to one-hot\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74436f74",
   "metadata": {},
   "source": [
    "# Defining even more layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Define your model -----\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')  # output units = number of classes\n",
    "])\n",
    "\n",
    "# ----- Compile your model -----\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ----- Callbacks -----\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d1434",
   "metadata": {},
   "source": [
    "# Training evaluating and saving this second iteration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fcce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Train your model -----\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ----- Evaluate on test data -----\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# ----- Save the model -----\n",
    "model.save(\"improved_model.h5\")\n",
    "print(\"Model saved as 'improved_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce6a14",
   "metadata": {},
   "source": [
    "# Anomaly Detection Using isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df0223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Anomaly Detection: Found 149457 anomalies out of 14945674 samples.\n"
     ]
    }
   ],
   "source": [
    "# Isolation Forest\n",
    "isoforest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso_preds = isoforest.fit_predict(X)\n",
    "\n",
    "# Convert to 0 (normal) and 1 (anomaly)\n",
    "iso_anomalies = np.where(iso_preds == -1, 1, 0)\n",
    "print(f\"\\n⚠️ Anomaly Detection: Found {np.sum(iso_anomalies)} anomalies out of {len(iso_anomalies)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f39cd",
   "metadata": {},
   "source": [
    "## Making test data and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5394fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved as 'scaler.joblib'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "\n",
    "# Initialize and fit the scaler only on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the fitted scaler for later use\n",
    "dump(scaler, 'scaler.joblib')\n",
    "print(\"Scaler saved as 'scaler.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1060fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure you are selecting the target column correctly\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlep_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# This should be a 1D array containing the target values\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlep_type\u001b[39m\u001b[38;5;124m'\u001b[39m]    \u001b[38;5;66;03m# This should also be a 1D array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Now apply LabelEncoder\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Make sure you are selecting the target column correctly\n",
    "y_train = y_train['lep_type'] \n",
    "y_test = y_test['lep_type']    \n",
    "\n",
    "# Now apply LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform the test data labels using the fitted encoder\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Save the label encoder for later use\n",
    "dump(label_encoder, 'label_encoder.joblib')\n",
    "print(\"Label Encoder saved as 'label_encoder.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcde057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"structured.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b3e681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lep_pt', 'lep_eta', 'lep_phi', 'lep_E', 'lep_charge', 'lep_type',\n",
      "       'jet_n', 'jet_pt', 'met_et', 'met_phi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209d9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vudut\\anaconda3\\envs\\tfamd\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\preprocessed_data.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(\"lep_type\", axis=1)  # replace \"label\" with your actual label column name\n",
    "y = df[\"lep_type\"]\n",
    "\n",
    "# Load the scaler and model\n",
    "scaler = load(\"scaler.joblib\")\n",
    "model = load_model(\"structured.h5\")\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 random test samples\n",
    "sample_indices = np.random.choice(len(X_scaled), size=5, replace=False)\n",
    "X_test_samples = X_scaled[sample_indices]\n",
    "y_test_samples = y.iloc[sample_indices].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20fe36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vudut\\anaconda3\\envs\\tfamd\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\vudut\\anaconda3\\envs\\tfamd\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\vudut\\anaconda3\\envs\\tfamd\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 690ms/step\n",
      "Sample 1:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    13.0\n",
      "---\n",
      "Sample 2:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    11.0\n",
      "---\n",
      "Sample 3:\n",
      "Predicted Class: 11.0\n",
      "Actual Class:    13.0\n",
      "---\n",
      "Sample 4:\n",
      "Predicted Class: 11.0\n",
      "Actual Class:    11.0\n",
      "---\n",
      "Sample 5:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    13.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load everything\n",
    "df = pd.read_csv(r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\preprocessed_data.csv\")\n",
    "X = df.drop(\"lep_type\", axis=1)\n",
    "y = df[\"lep_type\"]\n",
    "\n",
    "# Load the scaler, model, and label encoder\n",
    "scaler = load(\"scaler.joblib\")\n",
    "model = load_model(\"structured.h5\")\n",
    "label_encoder = load(\"label_encoder.joblib\")\n",
    "\n",
    "# Scale features\n",
    "X_scaled = scaler.transform(X.values)\n",
    "\n",
    "# Select random test samples\n",
    "sample_indices = np.random.choice(len(X_scaled), size=5, replace=False)\n",
    "X_test_samples = X_scaled[sample_indices]\n",
    "y_test_samples = y.iloc[sample_indices].values  # original labels for comparison\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_samples)\n",
    "predicted_classes_encoded = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Decode predicted class labels\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_classes_encoded)\n",
    "\n",
    "# Show predictions vs actual\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Predicted Class:\", predicted_classes[i])\n",
    "    print(\"Actual Class:   \", y_test_samples[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bcfcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved as 'scaler.joblib'\n",
      "Label Encoder saved as 'label_encoder.joblib'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vudut\\anaconda3\\envs\\tfamd\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 215ms/step\n",
      "Sample 1:\n",
      "Predicted Class: 11.0\n",
      "Actual Class:    11.0\n",
      "---\n",
      "Sample 2:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    13.0\n",
      "---\n",
      "Sample 3:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    13.0\n",
      "---\n",
      "Sample 4:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    11.0\n",
      "---\n",
      "Sample 5:\n",
      "Predicted Class: 13.0\n",
      "Actual Class:    13.0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from joblib import dump, load\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "##############################################\n",
    "# Part 1: Preprocessing & Saving Scaler/Encoder\n",
    "##############################################\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\preprocessed_data.csv\")\n",
    "\n",
    "# Fill missing values in 'jet_pt' using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['jet_pt'] = imputer.fit_transform(df[['jet_pt']])\n",
    "\n",
    "# Separate features and target\n",
    "# Here, 'lep_type' is the target column.\n",
    "X = df.drop(\"lep_type\", axis=1)\n",
    "y = df[\"lep_type\"]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scaling ---\n",
    "# Fit the StandardScaler only on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "dump(scaler, 'scaler.joblib')\n",
    "print(\"Scaler saved as 'scaler.joblib'\")\n",
    "\n",
    "# --- Label Encoding ---\n",
    "# Fit the LabelEncoder on the training target (ensure y_train is a 1D array)\n",
    "label_encoder = LabelEncoder()\n",
    "# If y_train is not already a Series, make sure to extract the column\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Save the LabelEncoder for later use\n",
    "dump(label_encoder, 'label_encoder.joblib')\n",
    "print(\"Label Encoder saved as 'label_encoder.joblib'\")\n",
    "\n",
    "##############################################\n",
    "# Part 2: Inference / Prediction\n",
    "##############################################\n",
    "\n",
    "# For inference we will use the entire CSV (you could also use your test set)\n",
    "df_inference = pd.read_csv(r\"C:\\Users\\vudut\\OneDrive\\Desktop\\Python\\MINI Project\\preprocessed_data.csv\")\n",
    "X_inference = df_inference.drop(\"lep_type\", axis=1)\n",
    "y_inference = df_inference[\"lep_type\"]\n",
    "\n",
    "# Load the saved scaler, trained model, and label encoder\n",
    "scaler = load(\"scaler.joblib\")\n",
    "model = load_model(\"structured.h5\")\n",
    "label_encoder = load(\"label_encoder.joblib\")\n",
    "\n",
    "# Scale the features using the loaded scaler (convert DataFrame to NumPy array)\n",
    "X_scaled = scaler.transform(X_inference.values)\n",
    "\n",
    "# Select 5 random samples from the data for testing predictions\n",
    "sample_indices = np.random.choice(len(X_scaled), size=5, replace=False)\n",
    "X_test_samples = X_scaled[sample_indices]\n",
    "y_test_samples = y_inference.iloc[sample_indices].values\n",
    "\n",
    "# Get predictions from the model\n",
    "predictions = model.predict(X_test_samples)\n",
    "\n",
    "# Convert predictions to the class indices\n",
    "predicted_classes_encoded = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Use the loaded LabelEncoder to decode class indices back to the original labels\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_classes_encoded)\n",
    "\n",
    "# Display predicted vs. actual for each sample\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Predicted Class:\", predicted_classes[i])\n",
    "    print(\"Actual Class:   \", y_test_samples[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac153ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfamd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
